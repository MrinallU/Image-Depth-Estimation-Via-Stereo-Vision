\documentclass[11pt]{scrartcl}
\usepackage{answers}
\Newassociation{sketch}{hintitem}{hints}
\renewcommand{\solutionextension}{out}
\usepackage{outlines}
\usepackage[sexy]{evan}
\newcommand\EE{\mathbb E}
\newcommand\PP{\mathbb P}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{graphicx}
\usepackage{wrapfig}
\begin{document}
\title{Image Depth Estimation Using Stereo Vision}
\author{Mrinall Umasudhan}
\date{April 25, 2022}
\maketitle
\Opensolutionfile{hints}

\begin{abstract}
  Word Count: \mailto{3986}.
\end{abstract}
%TOC
\tableofcontents

% EV.pdf
% ploh handouts

\newpage

\section{Introduction}
One of the most explored problems in the field of computer vision is the process
of accurately estimating the real-world depth of a pixel within a two dimensional
image. The inference of three dimensional information is done by using multiple two
dimensional views of a scene, the process being deemed the name stereo vision.

\subsection{Applications}
A common counter-argument to the practicality of stereo vision algorithms
are the presence of other sensors that do not make use of visual data such
as ultrasonic or time of flight distance sensors. While these sensors are not
not impacted by factors that would be detrimental to the accuracy of stereo
vision algorithms such as the lack of adequate lighting,
"stereo vision has the advantage that it achieves the 3-D acquisition without
energy emission or moving parts" (https://research.csiro.au/qi/stereo-vision/). Moreover,
whereas traditional distance sensors focus on a singular point in space, stereo vision
algorithms are only limited by the camera's field of view, making the depth analysis
large area far more simple and cost effective. Finally, stereo vision algorithms are able
to easily work in conjunction with other
computer vision techniques such as machine learning based object detection
models when compared to the previous depth estimation approaches as it already
tracks depth on the same image plane that a object detection model may be implemented on.
These factors allow for a far greater analysis of the
various shapes and angles in an image leading to its usage in various fields.
\\
\begin{wrapfigure}{R}{0.5\textwidth}
  \centering
  \includegraphics[width=0.5\textwidth]{img1.png}
  \caption{\label{fig:frog1}Stereo Vision For Road Deformity Detection}
\end{wrapfigure}

A common application of stereo vision algorithms is in the quality
control process of industrial factories. Factories must analyze each
finished product for deformities in order to maintain a standard of quality in their
products. However, many factories output a high volume of product every day meaning that
the human analysis of such product would be far too expensive and inefficient when
considering the large amounts of workers needed to manually inspect each products
as well as the time it takes for the inspection of a product. The installation of multiple
distance sensors in order to analyze each square inch of a product would also be far too expensive.
However, because a factory is in a controlled environment with uniform lighting and the object is one
of known shape, the usage of a stereo vision algorithm would be ideal for the situation as stereo cameras
are able to analyze objects with their large field of view and can easily detect deformities as the object
being analyzed is of known geometry, meaning the algorithm can compare each depth point of the current object
to the depth of a model product, reporting any deformities both accurately and efficiently.  and can easily detect deformities as the object
being analyzed is of known geometry, meaning the algorithm can compare each depth point of the current object
to the depth of a model product, reporting any deformities both accurately and efficiently. \\

A paper done by the University of Bristol explored deformity analysis preformed by stereo algorithms
further by acquiring three dimensional road data for autonomous cars further displaying the
capabilities of stereo algorithms. The process which this algorithm follows can be seen in the
figure above.

\subsection{Overview and Purpose}

The essence of many successful stereo vision algorithms can be summarized in three key steps:

 
  \paragraph{ Triangulation} The process of assigning depth values to each pixel in the image
        using multiple two dimensional views of a scene and the specific parameters from
        camera hardware, difference in location of the cameras used, and the disparity in the
        pixels from each view of the scene.
    \paragraph{ Calibration} The process of correcting image distortion caused by the
        spherical geometry of the camera lens
        and reifying the two dimensional views of the scene such that
        the objects in study are on the same plane.
  \paragraph{Pixel Correspondence} In order to apply the triangulation process the algorithm must be
        able to match a pixel from one view of the scene to another view taken from a separate camera
        also known as the disparity value of this pixel.
\\

\\

In modern research, the most studied step of the algorithm is the process of pixel Correspondence,
better known as stereo matching. As of now there are many optimization techniques being applied
to stereo matching algorithms in order to increase their efficiency and accuracy. Firstly, this paper explains
the math and logic behind each portion of a successful stereo vision system while providing implementations.  finding value in optimization techniques
when compared to more standard stereo matching approaches. In order to analyze and implement a sound stereo vision algorithm
as well as a optimized matching algorithm scholarly sources were regarding stereo vision and optimization
techniques for pixel correspondence. After implementing a standard matching algorithm as well as another using
the optimization technique known as dynamic programming, I found that there was a significant increase
in both accuracy and efficiency in the depth estimates provided from the algorithm.


\section{Triangulation}

The core of every stereo vision algorithm is to find the depth of a pixel
using multiple two dimensional views of the scene, more formally
this process is known as the backwards projection of a camera from image
coordinates into three dimensional world coordinates. In order to derive the
formulas for the backwards projection of a camera, the formulas by which
a camera uses in order to map world coordinates into image coordinates; this
process can be explained as the forwards projection model.
\subsection{Forward Projection Model}

Formally defined, the forward projection model "describes the mathematical relationship
between the coordinates of a point in three-dimensional space and its projection onto the image
plane of an ideal \textbf{pinhole camera}, where the camera aperture is described as a point and no lenses
are used to focus light" (\textbf{wikipedia}). The usage of a pinhole camera allows
for the elimination of lense distortion when mapping to the image plane, simplifying
the formulas significantly.

\begin{remark}
  The majority of cameras used in stereo vision algorithms, including those
  used in this paper, use lenses contradicting the pinhole camera model.
  However, because researchers calibrate their camera's in order to remove
  distortion from the images returned, the pinhole camera model can still
  be applied.
\end{remark}
The forward projection model of converting a 3D camera point into a 2D pixel
coordinate is defined using the formula below:
\begin{theorem}
  [Forward Projection Equation]

  \begin{displaymath}
    (u, v) = (f_x \cdot \displaystyle\frac{x_c}{z_c} + o_x,
    f_y \cdot \displaystyle\frac{y_c}{z_c} + o_y)
  \end{displaymath}
  \begin{figurekey}
    \begin{tabular}{llll}
      $(u,v)$ & two dimensional pixel coordinates        & $f_x$ & focal length on x-axis \\
      $x_c$   & $x$ position on scene coordinate frame   & $f_y$ & focal length on y-axis \\
      $y_c$   & $y$ position in scene coordinate frame   & $o_x$ & image center on x-axis \\
      $z_c$   & depth of point in scene coordinate frame & $o_y$ & image center on y-axis \\
    \end{tabular}
  \end{figurekey}
\end{theorem}
Whereas the other paramteres of the equation are self-explanatory, the focal
length ($f$) requires further explanation. The focal length is "the distance
between the lens and the image sensor when the subject is in focus" (Nikon Website).
As such, using this information the forward projection equations essentially show how
a ray from the camera to the scene is mapped to an image.


\subsection{Derivation of Backwards Projection Model}
It is clear that deriving the depth of a pixel from manipulating the forward
projection equations is impossible given the inquality in depth measurements
when using the $x$ and $y$ pixels. Therefore it is evident that additional
information is needed in order to infer depth. This is where the usage of
multiple viewpoints of a scene is needed.

\begin{remark}
  Although many stereo vision systems use more than two viewpoints of a scene,
  in order to simplify the implementation process a simple (binocular) stereo
  system will be used.
\end{remark}
\begin{wrapfigure}{R}{0.5\textwidth}
  \centering
  \includegraphics[width=0.3\textwidth]{img2.resized.png}
  \caption{\label{fig:frog2}Simple stereo system}
\end{wrapfigure}
\\
\\

As mentioned prior the forward projection equations essentially represent the camera
as projecting a ray from the image into a scene point. Using this fact, an
additional camera which is calibrated to be on the same plane as the original camera
may be used to project another ray from the corresponding image point onto the scene. By
finding the intersection of these two rays the depth of a pixel in the scene may be found. 
\\
Using the depth measurement of a pixel, it is also possible to derive
$(x, y)$ coordinates of a scene from the image coordinate frame, giving
the full scene coordinate frame:

\begin{theorem}
  [Backward Projection Equations]

  \begin{align}
    z & = \displaystyle\frac{b\cdot f_x}{(u_r-u_l)} \\
    x & = \displaystyle\frac{z}{f_x} \cdot (u_l - o_x) \\
    y & = \displaystyle\frac{z}{f_y} \cdot (v_l - o_y) 
  \end{align}

  \begin{figurekey}
    \begin{tabular}{llll}
      $(u_l,v_l)$ & pixel coordinates on left camera     & $f_x$ & focal length on x-axis \\
      $(u_r,v_r)$ & pixel coordinates on right camera   & $f_y$ & focal length on y-axis \\
      $x$     & $x$ position on scene coordinate frame  & $o_x$ & image center on x-axis \\
      $y$     & $y$ position in scene coordinate frame   & $o_y$ & image center on y-axis \\
        $z$     & depth of point in scene coordinate frame  &$b$ & baseline distance \\
    \end{tabular}
  \end{figurekey} \\ \\
    Note that the pixel coordinates in the left and right camera point at the same object in 
    the scene. However, because the cameras are located $b$ units away from each other, 
    the coordinates are not equal. The process of findinding the corresponding pixel in the 
    right camera for every pixel in the left camera, is known as the stereo correspondence 
    problem. 
\end{theorem}

The two rays projected from the camera, along with the calibrated basline measurement (distance
between left and right cameras) form a triangle, allowing for the derivations of the formulas
shown above. However, in order to attain the parameters in these formulas that are not 
immediatly present in the image such as focal length, and the baseline as well as correcting
for lense distortion camera calibration is required. Moreover, in order to make use of the ray 
projected from the additional viewpoint the corresponding pixel from the right camera must be 
found, this can be seen in the calculation of the depth as the $x$ value of the selected pixel 
in the right camera is subtracted by the $x$ value of the corresponding pixel of the left camera, this 
value is formally known as the disparity of a pixel $(u_r-u_l)$ and is essential to depth computation. 


\section{Camera Calibration}

In order for the triangulation formulas to apply all cameras in the stereo 
system must be calibrated to match the pinhole camera model. This involves
finding the hardware parameters of the camera and correcting the images 
returned for lense distortion. Moreover, the images must be transfored such 
that they are displayed parallel to each other. 

\subsection{Intrinsic matrix}

A key step in correcting for lense distortion and applying the triangulation formulas
is identifiying the instrinsic parameters of both cameras. Formally explained, the 
instrinsic parameters are the variables used in the forward projection equations in order
to map 3D scene coordinates into image coordinates, such as the focal length and optical
center of the image. The intrinsic parameters of a camera are mathematically contained in 
a 3 by 3 matrix known as the instrinsic matrix. The forward projection equation can be 
rewritted in matrix form to show this: 

\begin{theorem}[Forward Projection Equation Matrix Variation]
     \begin{displaymath}
    \begin{bmatrix}
      u \\
      v \\
      w    \end{bmatrix} &=
    \begin{bmatrix}
      f_x & 0 & o_x \\
      0 & f_y & o_y \\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      X \\
      Y \\
      Z
    \end{bmatrix} 
  \end{displaymath}
  \begin{figurekey}
    \begin{tabular}{llll}
      $(u,v)$ & two dimensional pixel coordinates        & $f_x$ & focal length on x-axis \\
      $X$   & $x$ position on scene coordinate frame   & $f_y$ & focal length on y-axis \\
      $Y$   & $y$ position in scene coordinate frame   & $o_x$ & image center on x-axis \\
      $Z$   & depth of point in scene coordinate frame & $o_y$ & image center on y-axis \\
    \end{tabular}
  \end{figurekey}

\end{theorem}

The parameters of the intrinsic matrix can be found by making use of the field of view 
and resolution of the camera of the camera, typically stated in the hardware specifications of the camera. The 
formulas are described below: 

\begin{theorem}
  [Instrinsic Matrix Calculation]

  \begin{align}
      f_x & = \displaystyle\frac{o_x}{\tan{\displaystyle\frac{a_x}{2}}} \\
    f_y & = \displaystyle\frac{o_y}{\tan{\displaystyle\frac{a_y}{2}}}  \\
    o_x & = \displaystyle\frac{r_x}{2} \\ 
    o_y & = \displaystyle\frac{r_y}{2} \\ 
  \end{align}

  \begin{figurekey}
    \begin{tabular}{llll}
      $(u_l,v_l)$ & pixel coordinates on left camera     & $f_x$ & focal length on x-axis \\
      $(u_r,v_r)$ & pixel coordinates on right camera   & $f_y$ & focal length on y-axis \\
      $x$     & $x$ position on scene coordinate frame  & $o_x$ & image center on x-axis \\
      $y$     & $y$ position in scene coordinate frame   & $o_y$ & image center on y-axis \\
        $z$     & depth of point in scene coordinate frame  &$b$ & baseline distance \\
    \end{tabular}
  \end{figurekey} \\ \\
    Note that the pixel coordinates in the left and right camera point at the same object in 
    the scene. However, because the cameras are located $b$ units away from each other, 
    the coordinates are unique creating the pixel coorespondance problem. 
\end{theorem}



\subsection{Extrinsic Parameters}

The extrinsic parameters of a camera refer to the position and orientation of the camera 
with respect to the world coordinate frame and corresponding cameras. In more complex 
stereo systems an extrinsic matrix is required, detailing the translation of the camera 
in the $x$, $y$, and $z$ axis as well as the roll, pitch, and yaw angles of the cameras. 
However, because a binocular stereo system is used, the two cameras are garunteed to 
be pointing in a straight line, while being on the same $y$ and $z$ axis leaving only 
a horizontal distance that can easily be manually computed. The baseline ($b$) is measured 
by finding the distance between the centers of the left and rightmost camera. 

\begin{remark}
    Becuase a simple stereo is used, the cameras are garunteed to be positioned 
    such the $y$ position of the camera is identical, removing the need of additional 
    calibration beyond the manual baseline measurement. 
\end{remark}

\subsection{Lens Distortion}
    %Images from: https://www.tangramvision.com/blog/camera-modeling-exploring-distortion-and-distortion-models-part-i#tangential-de-centering-distortions
     %Text from: https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html  
The simple stereo model assumes that both cameras in use do not contain lenses. 
However, in real-world situations lenses must be present in order to ensure pixel 
quality. This results in two types of distortion which must be accounted for when
calibrating cameras.  
\paragraph{Radial Distortion}
    This variation of distortion causes "straight lines in images to appear curved" (h)
    Moreover, as pixels begin to deviate from the image center, distortion increases rapidly, 
    causing significant drops in accuracy when querying depth. 
\paragraph{Tangential Distortion}
    As opposed to radial distortion, tangential distortion causes some areas in the image to appear 
    closer than others due to the misalignment of the lense from the image plane. 
\\
\begin{figure}[!htb]
    \centering
    \subfloat[\centering Radial]{{\includegraphics[width=5cm]{radial.png} }}%
    \qquad
    \subfloat[\centering Tangential]{{\includegraphics[width=5cm]{tang.png} }}%
    \label{fig:example}% 
    \caption{Impacts of Distortion on the Image Plane}
\end{figure}
\\ 

\subsection{Accounting for distortion}

The transformation process between raw and distorted pixels can be modelled using the 
formulas below: 

\begin{theorem}
    [Radial Distortion Equations] 
    \begin{align}
    x_{distorted} = x( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6) 
    \\ y_{distorted} = y( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6)    
    \end{align}
    
\end{theorem}


\begin{theorem}
    [Tangential Distortion Equations] 
    \begin{align}
        x_{distorted} = x + [ 2p_1xy + p_2(r^2+2x^2)]  
       \\ y_{distorted} = y + [ p_1(r^2+ 2y^2)+ 2p_2xy]
    \end{align}
    
\end{theorem}

As seen in the formulas the distortion is maginified through the following distortion coefficients: 
\begin{displaymath}
    Distortion \; coefficients=(k_1 \hspace{10pt} k_2 \hspace{10pt} p_1 \hspace{10pt} p_2 \hspace{10pt} k_3)
\end{displaymath}

By finding these components and transforming the pixels accordingly, the cameras are then 
completely calibrated. They are found by analyzing multiple images of known geometry and 
comparing the pixels in the camera with the real-world coordinates of the image. However, 
instead of completeing this process manually OpenCV, the framework being used, automates this 
process through built in functions, meaning only a conceptual understanding of lense distortion 
is needed to proceed. 

\section{Pixel Correspondence}

As mentioned prior, the pixel correspondence problem is the most studied are in the 
field of stereo vision and by extension, the main focus of this paper. The correspondence 
problem boils down to interating through each pixel in the left camera and finding the 
corresponding pixel in the right camera in a efficient and accurate manner. In this paper, 
two methods will be analyzed, a standard window based approach as well as a newer apprach 
using a optimization technique known as Dynamic Programming (DP) which in theory will increase 
the program's accuracy and runtime. After implementation, the 
two appraches will both be tested using the same stereo system in order to reveal differences 
in accuracy and speed in order to determine the value in the usage of optimization algorithms, 
such as DP in stereo vision. 

\subsection{Window Based SSD Disparity Estimation}
This method of finding corresponding pixels in two cameras uses a window based approach. 
Formally defined a window consists of a $n$ by $n$ grid of pixels. In order to find the 
corresponding pixel in another camera, the algorithm creates a fixed window the chosen pixel. 
Then on the corresponding row of the second camera, another window is created; however, 
this window is not fixed, rather the algorithm slides the window over every other pixel 
on the row of the second image. The window that is most simmilar with the fixed window 
of the first camera is noted at the corresponding pixel. This process is then repeated 
for every pixel in the image in order to apply the triangulation formulas and calculate the 
real-world depth of the scene. 
\\
\\
Rather than directly comparing individual pixels, using a window of pixels 
are less resistant to noise and have enough variation to form a pattern, 
esspecially when two individual pixels may easily have the same value. The image 
below shows the window iteration and comparison process across images described above:

\begin{figure}[!htb]
    \centering
    \subfloat{{\includegraphics[width=12cm]{SSD.jpg} }}%
    \caption{Window Based Correspondence Illustration}
\end{figure}


\begin{remark}
    Because the cameras have been calibrated, the two images are garunteed to lie 
    on the same y-axis, therefore the window only needs be transformed in one 
    direction. 
  % https://www.csd.uwo.ca/~oveksler/Courses/Winter2016/CS4442_9542b/L11-CV-stereo.pdf
\end{remark}
\\

\begin{wrapfigure}{R}{0.45\textwidth}
  \includegraphics[width=0.6\textwidth]{ex.jpg}
  \caption{\label{fig:frog1} Sample Window Cost Calculation}
\end{wrapfigure}

One segment of the algorithm that has not been explained is how the simmilarty is 
measured between two windows. Firstly, the image must be converted from the standard RGB format, 
a full colored image with each pixel consisting of three color channels: red, green, and blue,  
into a greyscale image. This is done such that each pixel is normalized from a value between 
0 and 255, representing the intensity of light shining on each pixel, so that mathmatical 
operations can be preformed easily without having to worry about the additional complexity 
created by accounting for three color channels. After this, the algorithm makes use of a sum of squared differences apprach
(SSD) to the windows, in which the squared difference is taken from each pixel in both windows. 
These differences are then summed and stored for the pixel centered in the right camera's window. The 
pixel in the right camera which has the minimmum cost is marked as the corresponding pixel 
for the pixel centered in the fixed window of the left camera. The cost computation process 
between two windows in the left and right images described above is shown by the
illustration on the right: 
\newpage

The window iteration process and cost computation method are applied to each pixel in the 
left image in order to attain a disparity map, a map linking each pixel in the left image 
to the corresponding pixel in the right image, is created. This process can be seen in the 
psuedocode below:
\begin{remark}
    The psuedocode below assumes a 3 by 3 window size as the code that will 
    be used to analyze preformance will utilize the same dimensions. 
\end{remark}
\begin{algorithm}[!htb]
    \floatname{algorithm}{Window Based Stereo Matching}
    \algrenewcommand\algorithmicrequire{\textbf{Input: image, rows, columns }}
    \algrenewcommand\algorithmicensure{\textbf{Output: disparityMap}}
    \caption{}
    \label{alg:}
    \begin{algorithmic}[1]
        \Require $input$
        \Ensure $output$ 
        \State $rows \gets rows-1$
        \State $columns \gets columns-1$
        \While{$rows \neq 0$}
            \State $curCollumn \gets 1$
            \While{$curCollumn \neq columns$}
                \State $minimmumCost \gets \infinity$
                \State $correspondingPixel \gets -1$
                \State $slidingWindow \gets 1$
                \While{$slidingWindow \neq columns$}
                      \State $cost \gets getCost(rows, slidingWindow, curCollumn)$
                      \If{$cost < minimmumCost$}
                         \State $minimmumCost \gets cost$
                         \State $correspondingPixel \gets curCollumn$
                      \EndIf
                \EndWhile
            \EndWhile 
            \State $disparityMap[rows][curCollumn] = correspondingPixel - curCollumn$
        \EndWhile
        \State \textbf{return} $disparityMap$
    \end{algorithmic}
\end{algorithm}

\subsubsection{Drawbacks}
% https://www.researchgate.net/publication/3192330_A_stereo_matching_algorithm_with_an_adaptive_window_Theory_and_experiment
% image from https://www.csd.uwo.ca/~oveksler/Courses/Winter2013/CS4442_9542b/L14-CV-stereo.pdf
\begin{wrapfigure}{R}{0.5\textwidth}
  \includegraphics[width=0.7\textwidth]{winsize.png}
    \caption{\label{fig:frog1} Window size compared to depth map (darker pixel shade refers to greater depth)}

\end{wrapfigure}


While this algorithm is easy to implement and offers a reasonable amount of accuracy, there 
are multiple shortcomings to the usage of this method. Firstly, creating multiple windows 
acorss images as well as iterating through multiple windows may take a significant, 
amount of processing time, especially as the resolution of the input image increases. 
Moreover, making use of windows creates a problem 
known as the window sizing problem. Formally defined the window problem states that 
the window size must be large enough to include enough intensity variation for reliable matching
, but small enough to avoid the effects of projective distortion". This means that when the 
window size is too small, a inaccurate disparity measurement is given as the window does
not account for differences in lighting around the pixel which may have significant effects on the cost function.
On the other hand, when the window size is too large the disparty measurement may become inaccurate as it ignores
the finer details in images and only gives singificance to bigger features. The figure on the right displays the 
affects which a smaller and larger window may have when estimating depth depth. Finally, there is no method to 
determine whether a pixel being searched for is occluded or present which may lead to 
significant error when computing depth. 

% https://www.youtube.com/watch?v=kxsvG4sSuvA
% http://www.cs.umd.edu/~djacobs/CMSC426/Slides/stereo_algo.pdf
\subsection{Dynamic Programming Based Disparity Estimation}
As opposed to the previously described window based SSD algorithm, the current algorithm 
in discussion makes use of a function which computes a "cost" of matching two pixels together. 
By minimizing the sum of this cost function for each pixel in the image, the corresponding pixel 
is found for every pixel in the image. This minimization process can be described belows:

\begin{theorem}[Cost Function Minimization]
    \begin{align}
        cost & =  (leftPixel - rightPixel)^2 \\
        d & = \displaystyle\sum_{leftPixel=1}^{N} cost(leftPixel,yPixel,disparity)
    \end{align}
    Note that the cost function can be arbitrarily assigned as the algorithm will 
    minimize the function regardless. In this paper the cost function will be equal to the differences between pixel intesities 
    will be squared, the same method which the window based algorithm uses except with the operation being formed 
    on a singular pixel. 
\end{theorem}

% https://www.ri.cmu.edu/pub_files/pub4/ohta_y_1985_1/ohta_y_1985_1.pdf
% http://www.cs.umd.edu/~djacobs/CMSC426/Slides/stereo_algo.pdf
It is known that the sum of the cost function in all pixels must be minimized, but how can 
the algorithm efficiently approach this problem? This is where Dynamic Programming (DP)
must be used. DP is a optimization technique which increases runtime by breaking a general 
problem into subproblems.  As mentioned previously, camera calibration garuntees that the corresponding 
pixel in the left camera must be found in the same row in the right image. This allows for the 
matching problem to transform into a well known weighted matching problem which is commonly  
solved by DP. Imagine forming a $n$ by $n$ grid with n being the number of pixels in row 
being studied in the image. This grid contains $n^2$ collumns with the cell in the 
$i$th row and the $j$th collumn containing the cost of matching the $i$th pixel in the 
left image's row with the $j$th pixel in the right image. By finding the shortest path, 
smallest sum of weights, from the bottom left corner to the top right corner of this 
grid the best possible matching configuration is found. DP solves this shortest path problem 
by first solving the problem for smaller subrectangles of the entire grid. Moreover, because 
the pixel matchings must be unique, "a feature in the left image can match to no more than one 
feature in the right image", and the ordering must be monotonic, meaning that the path cannot 
move backwards and must be moving in a diagnal manner. The method by which the algorithm constructs 
the optimal path by making use of optimal path calculations from smaller subrectangles is shown below: 
\[
    \texttt{dp}[i][j] =
\begin{cases}
    \max(\texttt{dp}[i-1][j], \texttt{dp}[i][j-1]), \texttt{dp}[i-1][j-1]) + intensity[i][j] \\

\end{cases}
\]
\begin{remark}
    Formally explained, the figure above states that given the last cell of the grid 
    is $(i,j)$, the first cell is $(1,1)$, and the path must not have any discontinuties, 
    the second to last cell must be one of the following: $(i-1, j-1)$ , $(i, j-1)$ , or 
    $(i-1, j)$. Thus by adding the value of cell $(i, j)$ to the paths ending on the previously
    mentioned cells, all paths ending on $(i,j)$ are constructed. As such, this process of 
    using optimal paths of previous subrectangles motivates the statement above. By repeating 
    this process for every pixel in the grid, an optimal matching pattern for every pixel from 
    $(1,1)$ to $(n,n)$. 
\end{remark}

\subsubsection{Advantages}

\begin{wrapfigure}{R}{0.5\textwidth}
  \includegraphics[width=0.4\textwidth]{dpG.png}
    \caption{\label{fig:frog1} Example of optimal pixel matching path found by DP algorithm}
\end{wrapfigure}
\\ 
There are many theoretical advantages of implememting
a DP based algorithm over a window based algorithm. 
As previously mentioned, DP allows for the reduction 
of runtime from a polynomial $n^x$ operations into 
a linear $n$ operations due to utilizing individual 
pixel intensities. Moreover, making use of individual pixels 
allows for the circumvention of the window sizing problems 
associated with the SSD based algorithm, increasing accuracy 
singnficantly. Finally, making use of DP allows the algorithm 
to recognize when a pixel has no reasonable matches. This can be detected 
as a valid path must be diagnal in nature; therefore when the pixel with the 
lowest cost forms a straight line with the pixel in question, the pixel is marked 
as occluded as including the pixel would lead to an unoptimal cost and by extension, 
a unoptimal matching arrangement. The figure to the right illustrates the concept 
of constructing an optimal path while detecting occlusions. 



\section{Implementation}
Now that the general components of the algorithm have been discussed, the implementation 
and testing process can be described. 

\subsection{Tools}

In order to create a rudimentary binocular strereo system, two logitech C270 
webcams will attatched to a wooden apparatus at 3 inches apart. The camera's 
will then be connected to a laptop in order to execute the program. In regards 
to the software aspect of implementation, the Python programming language will 
be used in conjunction with a widely used Computer Vision framework known as 
OpenCV which allows the program to query image data from both cameras. The 
overall structure of the code can be explained through the following outline:



\subsection{Testing}
In order to test the algorithm, I created several varying scenes with variable surroundings 
and lighting conditions in order to verify the speed and accuracy of which both correspondence
algorithms preformed when inffering the depth of all objects on the scene. For this paper 
major objects are classified as objects which are of singnificant size such that they are clearly 
distinct from the background of a image. The figure to the right gives an example of a scene used 
in testing. The outline below illustrates how all of the steps discussed in previous sections 
combine in order to measure preformance of the correspondence algorithms in question. 
\begin{outline}[enumerate]
   \1 Query images from left and right cameras.  
    \1 Calculate intrinsic and extrinsic parameters 
   \1 Correct for Image Distortion 
   \1 Start first timer 
   \1 Start first window based stereo correspondence algorithm 
   \1 End timer when correspondence algorithm is finished running and store time taken 
   \1 Start second timer
   \1 Run DP based correspondence algorithm 
   \1 End timer when algorithm is finished running and store time taken
   \1 Measure true accuracy rate compared with a purposefully placed object on the scene 
        to true depth. 
    \1 Store time taken and accuracy rates for data analysis. 
    \1 Repeat for all test cases. 
\end{outline}
\section{Results and Conclusion}

After running the algorithms against 20 various scenes, the following results were 
produced:
\\

\begin{table}[H]
\begin{tabular}{lll}
\hline
Scene & Window Correspondance Runtime (ms) & DP Correspondance Runtime (ms) \\ \hline
1     & 1300                               & 302                            \\
2     & 1173                               & 341                            \\
3     & 1420                               & 629                            \\
4     & 1912                               & 640                            \\
5     & 1341                               & 452                            \\
6     & 1119                               & 501                            \\
7     & 1132                               & 643                            \\
8     & 2016                               & 751                            \\
9     & 1251                               & 364                            \\
10    & 1762                               & 704                            \\ \hline
\end{tabular}
\end{table}

\\
\begin{table}[H]
\begin{tabular}{lll}
\hline
Scene & Window Correspondance Error (in) & DP Correspondance Error (in) \\ \hline
1     & 1.95                             & 0.34                         \\
2     & 2.40                             & 1.12                         \\
3     & 1.33                             & 0.78                         \\
4     & 2.45                             & 0.51                         \\
5     & 1.50                             & 1.20                         \\
6     & 2.04                             & 0.83                         \\
7     & 2.09                             & 1.33                         \\
8     & 2.41                             & 0.35                         \\
9     & 1.41                             & 0.47                         \\
10    & 2.27                             & 1.20                         \\ \hline
\end{tabular}
\end{table}

All in all, the results show that the DP based algorithm offers significantly lower 
runtimes while returning a more accurate depth measurement compared to the window 
based algorithm. The results align with the theoretical conlusion that a DP based 
appreach is superiour to the standard method. However, the DP solution did not 
eliminate error completely. This could have happened through human errors in measurement 
when verifying distance or when manually entering extrinsic parameters such as the 
baseline distance. Another possible explanation of such error may be associated with 
a issue of making use of a stereo algorithm in which variations of lighting causes 
misinterpretations of the cost function. As such, additional questions regarding the 
validity of the concept of stereo systems as a whole can be brought into question. 
There may be better methods of estimating image depth such as matching external sensor 
data with image pixels, such as cutting edge LiDAR sensors. However, stereo vision systems 
are able to offer low cost solutions to depth estimation in many situations and with 
the additional value of optimization techniques such as DP, stereo systems continue to be 
a feasible solution for attaining 3D understanding from images. 



\section{Appendix}


\section{Works Cited}


\end{document}
